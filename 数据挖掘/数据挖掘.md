
# 数据挖掘

------
## 绪论
1. 什么是数据挖掘
    是在大型数据存储库中，自动的发现有用信息的过程。
2. 数据挖掘要解决的问题
    1. 可伸缩 数据量太大，只有可伸缩才能以有效的方式访问每个记录。
    2. 高维性 随着维度（特征数）的增加，计算复杂性迅速增加。
    3. 异种数据和复杂数据  非传统数据类型
    4. 数据的所有权和分布 数据分布在不同的地方，需要统一获取。
    5. 非传统的分析 自动地产生和评估假设。
3. 数据挖掘任务
    1. 预测任务
    2. 描述任务

## 数据
1. 数据预处理
    1. 聚集：将2个或者多个对象合并成单个对象。
    2. 抽样：选择数据对象子集进行分析。
    3. 维归约：通过创建新属性，将一些旧属性合并在一起来降低数据集的维度。
    4. 特征子集选择:仅使用特征的一个子集来降低维度。
    5. 特征创建：由原来的属性创建新的属性。
    6. 离散化和二元化：某些分类算法，要求数据是分类属性形式。
    7. 变量变换：对变量的所有值进行转换。
2. 相似性和相异性
    1. 相似性：两个对象相似程度的数值度量
    2. 相异性：两个对象差异程度的数值度量
    
## 探索数据
1. 汇总统计
    1.  频数和众数：特定数据集中每个值出现的频率叫频数，众数是具有最高频率的值。
    2.  均值和中位数
    3.  极差和方差：极差表示最大散步（最大值-最小值），标准差是方差的平方根。

## 分类：基本概念、决策树与模型评估

1. 定义
   分类任务就是通过学习得到一个目标函数f，把每个属性x映射到一个预先定义的类标号y。
   目标函数也称分类模型。
   分类模型可以用于以下目的。
   1. 描述性建模。可以作为解释性的工具，用于区分不同类中的对象。
   2. 预测性建模。用于预测未知记录的类标号。
2. 解决分类问题的一般方法
   1. 首先，需要一个训练集。有类标号已知的记录组成
   2. 建立分类模型
   3. 将模型运用于检验集，应用模型。
3. 决策树归纳
   定义：决策树（Decision Tree）是一种简单但是广泛使用的分类器。通过训练数据构建决策树，可以高效的对未知的数据进行分类。
   优点：
   1. 决策树模型可以读性好，具有描述性，有助于人工分析；
   2. 效率高，决策树只需要一次构建，反复使用，每一次预测的最大计算次数不超过决策树的深度。
   建立决策树
   1. 开始，所有记录看作一个节点
   2. 遍历每个变量的每一种分割方式，找到最好的分割点
   3. 分割成两个节点N1和N2
   4. 对N1和N2分别继续执行2-3步，直到每个节点足够“纯”为止

   属性类型
      1. 二元属性。 产生两个可能的输出。
      2. 标称属性。 
         1. 多路划分。 输出数取决于属性的不同属性个数。
         2. 二元划分。 多个属性，只有2个输出。
      3. 序数属性。 
         1. 二元或者多路划分。
      4. 连续属性。
         1. 多路划分。
   选择最佳划分的度量
      纯度。p(i|t)表示给定节点t中属于类i的记录所占的比例。有时省略节点t，直接用pi表示该比例。任意节点的类分布都可以记作(p0,p1),其中p1=1-p0。p0和p1的差值表示节点的不纯性的程度，差值越大，不纯度越小。进行属性划分时尽量选择纯度高的。
      1. 二元属性的划分。分别计算2个属性的增益，选择总和最小的。
      2. 标称属性的划分。分别计算各路属性的增益，选择总和最小的。
      3. 连续属性的划分。排序取中间值计算增益。
   停止条件
一种最直观的方式是当每个子节点只有一种类型的记录时停止，但是这样往往会使得树的节点过多，导致过拟合问题（Overfitting）。另一种可行的方法是当前节点中的记录数低于一个最小的阀值，那么就停止分割，将max(P(i))对应的分类作为当前叶节点的分类。
   过渡拟合
•噪音数据
•缺少代表性数据
•多重比较
