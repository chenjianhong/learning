
# 决策树
##定义：
  决策树（Decision Tree）是一种简单但是广泛使用的分类器。通过训练数据构建决策树，可以高效的对未知的数据进行分类。
##优点：
   1. 决策树模型可以读性好，具有描述性，有助于人工分析；
   2. 效率高，决策树只需要一次构建，反复使用，每一次预测的最大计算次数不超过决策树的深度。
   建立决策树
   1. 开始，所有记录看作一个节点
   2. 遍历每个变量的每一种分割方式，找到最好的分割点
   3. 分割成两个节点N1和N2
   4. 对N1和N2分别继续执行2-3步，直到每个节点足够“纯”为止

##属性类型
1. 二元属性。 产生两个可能的输出。
2. 标称属性。 
         1. 多路划分。 输出数取决于属性的不同属性个数。
         2. 二元划分。 多个属性，只有2个输出。
3. 序数属性。 
         1. 二元或者多路划分。
4. 连续属性。
         1. 多路划分。
##选择最佳划分的度量
1. 纯度。p(i|t)表示给定节点t中属于类i的记录所占的比例。有时省略节点t，直接用pi表示该比例。任意节点的类分布都可以记作(p0,p1),其中p1=1-p0。p0和p1的差值表示节点的不纯性的程度，差值越大，不纯度越小。
2. 进行属性划分时尽量选择纯度高的。
         1. 二元属性的划分。分别计算2个属性的增益，选择总和最小的。
         2. 标称属性的划分。分别计算各路属性的增益，选择总和最小的。
         3. 连续属性的划分。排序取中间值计算增益。
##停止条件
一种最直观的方式是当每个子节点只有一种类型的记录时停止，但是这样往往会使得树的节点过多，导致过拟合问题（Overfitting）。另一种可行的方法是当前节点中的记录数低于一个最小的阀值，那么就停止分割，将max(P(i))对应的分类作为当前叶节点的分类。
##过渡拟合
•噪音数据
•缺少代表性数据
•多重比较
